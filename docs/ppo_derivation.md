# PPO 推导

PPO 只是 RLHF 这个更大框架中的一个组件，是实现 RLHF 的一种具体手段，而不是唯一选择。理论上，其他 RL 算法（如 TRPO、SAC 等）也可以用在 RLHF 中，只是 PPO 因其优良特性成为了最常用的选择。

过去有一个学习笔记文档, 见 https://swze06osuex.feishu.cn/docx/HmChdV45boA5bVxKcVxcUUFZnZX

## 1. 系统组件

### 1.1 核心模型
1. **SFT Model $\pi^{SFT}$**：
   - 通过监督学习得到的基础模型
   - 用于计算与初始策略的 KL 散度

2. **Policy LM $\pi_{\theta}^{RL}$**：
   - 当前正在优化的策略模型
   - 基于 PPO 更新参数

3. **Reward Model $r(x,y)$**：
   - 从人类偏好数据学习的奖励模型
   - 为每个生成的回复打分

4. **Value Model $V_{\phi}(s_t)$**：
   - 评估状态价值
   - 用于计算优势函数

### 1.2 参考模型
- **Reference Policy $\pi_{\theta_{old}}^{RL}$**：
  - 用于计算重要性采样比
  - 定期从当前策略更新

## 2. 数据流程

### 2.1 输入处理
1. **用户查询**：
   - 接收用户输入 $x$
   - 将长序列分解为多个时间步

2. **状态表示**：
   - $s_t$ 包含：
     - 当前输入 $x$
     - 已生成的 tokens $y_1,...,y_{t-1}$

### 2.2 奖励计算
1. **即时奖励**：
   - 从奖励模型获得：$r(s_t,a_t)$
   - 考虑 KL 惩罚：$r(x,y) - \beta \text{KL}(\pi^{RL}||\pi^{SFT})$

2. **优势估计 (GAE)**：
   $$
   \hat{A}(s_t,a_t) = \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}
   $$
   其中：
   - $\delta_t = r(s_t,a_t) + \gamma V(s_{t+1}) - V(s_t)$
   - $\hat{R}_t = \hat{A}(s_t,a_t) + V(s_t)$

## 3. 训练目标

### 3.1 PPO-Clip 损失
$$
L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]
$$
其中：
- $r_t(\theta) = \frac{\pi_{\theta}^{RL}(a_t|s_t)}{\pi_{\theta_{old}}^{RL}(a_t|s_t)}$

### 3.2 值函数损失
$$
L^{VF}(\phi) = \mathbb{E}_t[(V_\phi(s_t) - \hat{R}_t)^2]
$$

### 3.3 语言模型损失
$$
L^{LM} = -\log \pi_{\theta}^{RL}(a_t|s_t)
$$

### 3.4 完整目标
$$
L^{TOTAL} = L^{CLIP} + c_1L^{VF} + c_2L^{LM}
$$

## 4. 训练流程

### 4.1 数据收集
1. 使用当前策略 $\pi_{\theta}^{RL}$ 生成回复
2. 通过奖励模型计算奖励
3. 计算优势估计
4. 存储到经验缓冲区

### 4.2 策略更新
1. 从经验缓冲区采样数据
2. 计算所有损失项
3. 更新策略网络和值网络
4. 定期更新参考策略

### 4.3 预训练数据利用
- 在训练过程中混合使用预训练数据
- 通过 $L^{LM}$ 保持语言生成能力

## 5. 实现要点

### 5.1 关键考虑
1. **KL 控制**：
   - 防止策略过度偏离 SFT 模型
   - 动态调整 KL 惩罚系数

2. **奖励缩放**：
   - 合理设置奖励范围
   - 考虑奖励归一化

3. **批处理策略**：
   - 高效处理长序列
   - 合理设置截断长度

### 5.2 稳定性技巧
1. **梯度裁剪**
2. **学习率调度**
3. **经验回放比例**
4. **值函数更新频率**

## 6. 与标准 PPO 的区别

### 6.1 主要创新
1. **引入奖励模型**：
   - 替代传统环境奖励
   - 从人类偏好学习

2. **KL 约束**：
   - 与 SFT 模型保持一定距离
   - 保持语言能力

3. **混合训练目标**：
   - 结合 RL 和 LM 损失
   - 平衡探索和稳定性

### 6.2 特殊挑战
1. **奖励稀疏性**：
   - 只在序列末尾有完整奖励
   - 需要合理分配中间奖励

2. **训练稳定性**：
   - 语言模型容易退化
   - 需要仔细平衡各项损失

3. **计算效率**：
   - 需要处理长序列
   - 计算和存储开销大 