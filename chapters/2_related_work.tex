\section{Related Work}
\label{Related Work}
\textbf{Reinforcement Learning from Human Feedback.}
Despite RLHF's effectiveness in aligning language models (LMs) with human values \cite{ChristianoLBMLA17, Bai2022training, llama2, instructGPT, fangjf2024neuron, FangZW00LWD024}, its complexity and resource demands have spurred the exploration of alternatives. RAFT \cite{RAFT} selects optimal training samples via an existing reward model, whereas RRHF \cite{RRHF} employs a simpler ranking loss, retaining PPO's efficiency. Diverse from these, DPO \cite{DPO} directly optimizes LMs using a preference-based loss function, showcasing enhanced training stability in comparison to traditional RLHF. Innovatively, SLiC-HF \cite{SLiC-HF} and KTO \cite{KTO} devise loss functions rooted in human decision-making, focusing on preference calibration and utility optimization, respectively. Dr. DPO \citep{drdpo} consider robust settings where safety or group information is known at training time. Further, RSO \cite{RSO} and ORPO \cite{ORPO} introduce efficient preference modeling and optimization, with ORPO uniquely combining supervised fine-tuning and preference alignment. These advancements reflect the ongoing shift towards more efficient, nuanced RL methods.

\textbf{Data Quality in LLM's Alignment.}
Recent studies have increasingly recognized the significance of data quality in the alignment of LLMs. For example, LIMA \cite{LIMA} leverages heuristics such as post scores, response lengths, formatting, and topics to manually craft 1000 high-quality datasets from StackExchange, wikiHow, and Reddit for superficial alignment. In a similar vein, \citet{Bai2022training} prioritize data points based on user engagement levels for dataset assembly. Rejection Sampling (RS) and Best-of-$N$ (BoN) techniques, as evidenced in the works of \citet{WebGPT} and \citet{Gao23ICML}, involve selecting the optimal candidate from $N$ generated possibilities through the application of a reward model. To enhance preference optimization, RSO \cite{RSO} uses statistical weightings to differentiate outcomes from an optimal policy and a base SFT policy. Besides, fDPO \cite{morimura2024filtered} employs a Reward Model to filter out low-quality data, effectively addressing dataset quality concerns.

% 越来越多的工作意识到LLM's alignment中数据质量的重要性，如LIMA utilized post score, response length, formatting, topic, and other heuristics to manually select content from StackExchange, wikiHow, and Reddit to formulate 1000 high-quality training datasets for superficial alignment. Similarly, Bai et al. (2022a) and Lambert et al. (2023b) weigh data points based on a measure of minimum engagement from users when creating their respective datasets.  In rejection sampling (RS), or Best-of-N sampling (BoN) as used in Nakano et al. (2021); Gao et al. (2023a), n candidate outputs are sampled from the generative model and the reward model selects the best candidate. To  akin to DPO, Liu et al. (2024b) performs a rejection-sampling-like optimization with statistical weightings predicting the likelihood of whether a completion came from the optimal policy or base SFT policy to improve downstream preference optimization similar to DPO or Sequence Likelihood Calibration (SLiC). fDPO uses an RM to identify and discard lower-quality data, effectively addressing the dataset quality issue. 

% \textbf{Reinforcement Learning from Human Feedback.}. RLHF has been increasingly recognized for its potential in aligning language models (LMs) with human values, thereby addressing issues related to biased or inaccurate outputs \cite{ChristianoLBMLA17, Bai2022training, llama2, instructGPT}. Despite its promise, the inherent complexity, stability challenges, and extensive memory requirements of RLHF paved the way for alternative methodologies aimed at streamlining the human feedback integration process.

% Alternative approaches include RAFT \cite{RAFT}, which tailors training samples selection using an existing reward model to optimize for preferred outputs, and RRHF \cite{RRHF}, which simplifies alignment by adopting a ranking loss, maintaining performance through policy optimization techniques like PPO. Diverse from these, DPO \cite{DPO} directly optimizes LMs using a preference-based loss function, showcasing enhanced stability and reduced computational overhead in comparison to traditional RLHF. Further exploring the landscape, SLiC-HF \cite{SLiC-HF} and KTO \cite{KTO} introduce novel loss functions aimed at preference calibration and utility maximization, derived from human decision-making principles. Notably, SLiC-HF focuses on margin calibration between comparison winners and losers, while KTO taps into the Kahneman-Tversky utility framework for implicit response scoring. Lastly, RSO \cite{RSO} and ORPO \cite{ORPO} represent innovative strides towards integrating preference models and optimization in more efficient frameworks. RSO leverages preference models for generating and optimizing sampled preference pairs, whereas ORPO proposes a synchronous approach to supervised fine-tuning and preference alignment without relying on an intermediate policy reference.

% \textbf{Reinforcement Learning from Human Feedback.} RLHF \cite{ChristianoLBMLA17, Bai2022training, llama2, instructGPT} has emerged as a key method for aligning language models with human values and preferences, mitigating the generation of biased or factually incorrect outputs. Nonetheless, RLHF is more complex than supervised learning, less stable, and requires more memory resources. These challenges has motivated the development in alternatives to the RLHF pipeline. For example, RAFT \cite{RAFT} uses an existing reward model to select the best set of training samples based on the model outputs, while RRHF \cite{RRHF} leverages a much simpler ranking loss to align human preferences and retain the performance of PPO. DPO \cite{DPO} is another alternative to RLHF that uses a preference loss function to directly optimize the LLMs, and has been shown to be more stable and less computationally intensive than RLHF. In a similar spirit, \citet{SLiC-HF} proposed to calibrate the score so that the score of the winner in comparison has a margin over the score of the loser, and induces a different SLic loss. Similarly, \citet{KTO} derived a different loss function (called KTO) from the Kahneman-Tversky human utility function, which implicitly denotes a score of the given response. \citet{RSO} proposed Rejection Sampling Optimization (RSO) which utilizes a preference model to generate preference pairs with candidates sampled from the optimal policy; then preference optimization is applied on the sampled preference pairs. \citet{ORPO} proposed Odds Ratio Preference Optimization (ORPO) algorithm that can perform supervised fine-tuning and preference alignment in one training session without maintaining an intermediatereference policy.

% Despite these efforts, all the methods ignore the presence of noise in the training data, which can lead to suboptimal performance.

% The integration of generative models with human feedback has emerged as a powerful approach to enhance the utility, accuracy, and ethicality of generated content, fulfilling a range of desired outcomes \cite{instructGPT}. 
% Among the various alignment strategies, Reinforcement Learning from Human Feedback (RLHF) marks a pivotal advancement, which refines Large Language Models (LLMs) through the application of the Proximal Policy Optimization (PPO) algorithm \cite{PPO}. However, its foundation in Reinforcement Learning (RL) methodologies introduces inherent challenges, notably in the arena of training stability.
% In response to these challenges, Direct Preference Optimization (DPO) \cite{DPO} offers a novel paradigm by circumventing the stage of explicit reward model formation. Implicitly mirroring the ambitions of RLHF, DPO operationalizes by identifying an optimal surrogate for each individual instance while concurrently adjudicating between preferences on a pairwise basis. This methodology heralds a significant advancement in terms of simplification and the stabilization of the training trajectory \cite{dpo_app}.