\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[AI@Meta(2024)]{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card.
\newblock 2024.
\newblock URL \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

\bibitem[Azar et~al.(2023)Azar, Rowland, Piot, Guo, Calandriello, Valko, and Munos]{ipo}
M.~G. Azar, M.~Rowland, B.~Piot, D.~Guo, D.~Calandriello, M.~Valko, and R.~Munos.
\newblock A general theoretical paradigm to understand learning from human preferences.
\newblock \emph{CoRR}, abs/2310.12036, 2023.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, Showk, Elhage, Hatfield{-}Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda, Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and Kaplan]{Bai2022training}
Y.~Bai, A.~Jones, K.~Ndousse, A.~Askell, A.~Chen, N.~DasSarma, D.~Drain, S.~Fort, D.~Ganguli, T.~Henighan, N.~Joseph, S.~Kadavath, J.~Kernion, T.~Conerly, S.~E. Showk, N.~Elhage, Z.~Hatfield{-}Dodds, D.~Hernandez, T.~Hume, S.~Johnston, S.~Kravec, L.~Lovitt, N.~Nanda, C.~Olsson, D.~Amodei, T.~B. Brown, J.~Clark, S.~McCandlish, C.~Olah, B.~Mann, and J.~Kaplan.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{CoRR}, abs/2204.05862, 2022.

\bibitem[Bejan et~al.(2023)Bejan, Sokolov, and Filippova]{outlier_dataset}
I.~Bejan, A.~Sokolov, and K.~Filippova.
\newblock Make every example count: On the stability and utility of self-influence for learning from noisy {NLP} datasets.
\newblock In \emph{{EMNLP}}, 2023.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der Wal]{pythia}
S.~Biderman, H.~Schoelkopf, Q.~G. Anthony, H.~Bradley, K.~O'Brien, E.~Hallahan, M.~A. Khan, S.~Purohit, U.~S. Prashanth, E.~Raff, A.~Skowron, L.~Sutawika, and O.~van~der Wal.
\newblock Pythia: {A} suite for analyzing large language models across training and scaling.
\newblock In \emph{{ICML}}, 2023.

\bibitem[Bottou(2010)]{bottou2010large}
L.~Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{{COMPSTAT}}, 2010.

\bibitem[Bradley and Terry(1952)]{bradley1952rank}
R.~A. Bradley and M.~E. Terry.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, Nori, Palangi, Ribeiro, and Zhang]{GPT4_2}
S.~Bubeck, V.~Chandrasekaran, R.~Eldan, J.~Gehrke, E.~Horvitz, E.~Kamar, P.~Lee, Y.~T. Lee, Y.~Li, S.~M. Lundberg, H.~Nori, H.~Palangi, M.~T. Ribeiro, and Y.~Zhang.
\newblock Sparks of artificial general intelligence: Early experiments with {GPT-4}.
\newblock \emph{CoRR}, abs/2303.12712, 2023.

\bibitem[Chen et~al.(2024)Chen, Deng, Yuan, Ji, and Gu]{spin}
Z.~Chen, Y.~Deng, H.~Yuan, K.~Ji, and Q.~Gu.
\newblock Self-play fine-tuning converts weak language models to strong language models.
\newblock In \emph{{ICML}}, 2024.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{ChristianoLBMLA17}
P.~F. Christiano, J.~Leike, T.~B. Brown, M.~Martic, S.~Legg, and D.~Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{{NeurIPS}}, 2017.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou]{Ding2023EnhancingCL}
N.~Ding, Y.~Chen, B.~Xu, Y.~Qin, Z.~Zheng, S.~Hu, Z.~Liu, M.~Sun, and B.~Zhou.
\newblock Enhancing chat language models by scaling high-quality instructional conversations.
\newblock In \emph{EMNLP}, 2023.

\bibitem[Dong et~al.(2023)Dong, Xiong, Goyal, Pan, Diao, Zhang, Shum, and Zhang]{RAFT}
H.~Dong, W.~Xiong, D.~Goyal, R.~Pan, S.~Diao, J.~Zhang, K.~Shum, and T.~Zhang.
\newblock {RAFT:} reward ranked finetuning for generative foundation model alignment.
\newblock \emph{CoRR}, abs/2304.06767, 2023.

\bibitem[Ethayarajh et~al.(2024)Ethayarajh, Xu, Muennighoff, Jurafsky, and Kiela]{KTO}
K.~Ethayarajh, W.~Xu, N.~Muennighoff, D.~Jurafsky, and D.~Kiela.
\newblock {KTO:} model alignment as prospect theoretic optimization.
\newblock \emph{CoRR}, abs/2402.01306, 2024.

\bibitem[Fang et~al.(2024{\natexlab{a}})Fang, Bi, Wang, Jiang, Gao, Wang, Zhang, Shi, Wang, and Chua]{fangjf2024neuron}
J.~Fang, Z.~Bi, R.~Wang, H.~Jiang, Y.~Gao, K.~Wang, A.~Zhang, J.~Shi, X.~Wang, and T.-S. Chua.
\newblock Towards neuron attributions in multi-modal large language models.
\newblock In \emph{Thirty-eighth Conference on Neural Information Processing Systems}, 2024{\natexlab{a}}.

\bibitem[Fang et~al.(2024{\natexlab{b}})Fang, Zhang, Wu, Yang, Liu, Li, Wang, Du, and Wang]{FangZW00LWD024}
J.~Fang, S.~Zhang, C.~Wu, Z.~Yang, Z.~Liu, S.~Li, K.~Wang, W.~Du, and X.~Wang.
\newblock Moltc: Towards molecular relational modeling in language models.
\newblock In \emph{{ACL} (Findings)}, pages 1943--1958. Association for Computational Linguistics, 2024{\natexlab{b}}.

\bibitem[Gao et~al.(2023)Gao, Schulman, and Hilton]{Gao23ICML}
L.~Gao, J.~Schulman, and J.~Hilton.
\newblock Scaling laws for reward model overoptimization.
\newblock In \emph{{ICML}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 10835--10866. {PMLR}, 2023.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
R.~Ge, F.~Huang, C.~Jin, and Y.~Yuan.
\newblock Escaping from saddle points - online stochastic gradient for tensor decomposition.
\newblock In \emph{{COLT}}, 2015.

\bibitem[Havrilla et~al.(2023)Havrilla, Zhuravinskyi, Phung, Tiwari, Tow, Biderman, Anthony, and Castricato]{havrilla-etal-2023-trlx}
A.~Havrilla, M.~Zhuravinskyi, D.~Phung, A.~Tiwari, J.~Tow, S.~Biderman, Q.~Anthony, and L.~Castricato.
\newblock trl{X}: A framework for large scale reinforcement learning from human feedback.
\newblock In \emph{EMNLP}, Dec. 2023.

\bibitem[Hong et~al.(2024)Hong, Lee, and Thorne]{ORPO}
J.~Hong, N.~Lee, and J.~Thorne.
\newblock {ORPO:} monolithic preference optimization without reference model.
\newblock \emph{CoRR}, abs/2403.07691, 2024.

\bibitem[Huang et~al.(2023)Huang, Chen, Wen, Zhang, Li, Wang, and Chen]{MACL}
Z.~Huang, H.~Chen, Z.~Wen, C.~Zhang, H.~Li, B.~Wang, and C.~Chen.
\newblock Model-aware contrastive learning: Towards escaping the dilemmas.
\newblock In \emph{{ICML}}, 2023.

\bibitem[Jiang et~al.(2023{\natexlab{a}})Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~Las~Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{Jiang2023Mistral7}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~de~Las~Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, L.~R. Lavaud, M.-A. Lachaux, P.~Stock, T.~L. Scao, T.~Lavril, T.~Wang, T.~Lacroix, and W.~E. Sayed.
\newblock Mistral {7B}.
\newblock \emph{ArXiv}, abs/2310.06825, 2023{\natexlab{a}}.

\bibitem[Jiang et~al.(2023{\natexlab{b}})Jiang, Ren, and Lin]{Jiang2023LLMBlenderEL}
D.~Jiang, X.~Ren, and B.~Y. Lin.
\newblock {LLM-Blender}: Ensembling large language models with pairwise ranking and generative fusion.
\newblock In \emph{ACL}, 2023{\natexlab{b}}.

\bibitem[Kukleva et~al.(2023)Kukleva, B{\"{o}}hle, Schiele, Kuehne, and Rupprecht]{long_tail}
A.~Kukleva, M.~B{\"{o}}hle, B.~Schiele, H.~Kuehne, and C.~Rupprecht.
\newblock Temperature schedules for self-supervised contrastive methods on long-tail data.
\newblock In \emph{{ICLR}}, 2023.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra]{mae}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa, D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{{ICLR} (Poster)}, 2016.

\bibitem[Liu et~al.(2023)Liu, Zhao, Joshi, Khalman, Saleh, Liu, and Liu]{RSO}
T.~Liu, Y.~Zhao, R.~Joshi, M.~Khalman, M.~Saleh, P.~J. Liu, and J.~Liu.
\newblock Statistical rejection sampling improves preference optimization.
\newblock \emph{CoRR}, abs/2309.06657, 2023.

\bibitem[Meng et~al.(2024)Meng, Xia, and Chen]{SimPO2024}
Y.~Meng, M.~Xia, and D.~Chen.
\newblock Simpo: Simple preference optimization with a reference-free reward.
\newblock \emph{CoRR}, abs/2405.14734, 2024.

\bibitem[Morimura et~al.(2024)Morimura, Sakamoto, Jinnai, Abe, and Air]{morimura2024filtered}
T.~Morimura, M.~Sakamoto, Y.~Jinnai, K.~Abe, and K.~Air.
\newblock Filtered direct preference optimization.
\newblock \emph{arXiv preprint arXiv:2404.13846}, 2024.

\bibitem[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse, Jain, Kosaraju, Saunders, Jiang, Cobbe, Eloundou, Krueger, Button, Knight, Chess, and Schulman]{WebGPT}
R.~Nakano, J.~Hilton, S.~Balaji, J.~Wu, L.~Ouyang, C.~Kim, C.~Hesse, S.~Jain, V.~Kosaraju, W.~Saunders, X.~Jiang, K.~Cobbe, T.~Eloundou, G.~Krueger, K.~Button, M.~Knight, B.~Chess, and J.~Schulman.
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock \emph{CoRR}, abs/2112.09332, 2021.

\bibitem[OpenAI(2023)]{GPT4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, abs/2303.08774, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{instructGPT}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~L. Wainwright, P.~Mishkin, C.~Zhang, S.~Agarwal, K.~Slama, A.~Ray, J.~Schulman, J.~Hilton, F.~Kelton, L.~Miller, M.~Simens, A.~Askell, P.~Welinder, P.~F. Christiano, J.~Leike, and R.~Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Alobeidli, Cappelli, Pannier, Almazrouei, and Launay]{mixture_data}
G.~Penedo, Q.~Malartic, D.~Hesslow, R.~Cojocaru, H.~Alobeidli, A.~Cappelli, B.~Pannier, E.~Almazrouei, and J.~Launay.
\newblock The refinedweb dataset for falcon {LLM:} outperforming curated corpora with web data only.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Pruthi et~al.(2020)Pruthi, Liu, Kale, and Sundararajan]{pruthi2020estimating}
G.~Pruthi, F.~Liu, S.~Kale, and M.~Sundararajan.
\newblock Estimating training data influence by tracing gradient descent.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Pukelsheim(1994)]{3sigma}
F.~Pukelsheim.
\newblock The three sigma rule.
\newblock \emph{The American Statistician}, 1994.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{DPO}
R.~Rafailov, A.~Sharma, E.~Mitchell, C.~D. Manning, S.~Ermon, and C.~Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock 1951.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{PPO}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{tldr2}
N.~Stiennon, L.~Ouyang, J.~Wu, D.~M. Ziegler, R.~Lowe, C.~Voss, A.~Radford, D.~Amodei, and P.~F. Christiano.
\newblock Learning to summarize from human feedback.
\newblock \emph{CoRR}, abs/2009.01325, 2020.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton{-}Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~Canton{-}Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu, B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou, H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S. Koura, M.~Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao, X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton, J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith, R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan, P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang, A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{CoRR}, abs/2307.09288, 2023.

\bibitem[V{\"{o}}lske et~al.(2017)V{\"{o}}lske, Potthast, Syed, and Stein]{tldr_dataset}
M.~V{\"{o}}lske, M.~Potthast, S.~Syed, and B.~Stein.
\newblock Tl;dr: Mining reddit to learn automatic summarization.
\newblock In \emph{NFiS@EMNLP}, 2017.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Jiang, Yang, Liu, and Chen]{f-dpo}
C.~Wang, Y.~Jiang, C.~Yang, H.~Liu, and Y.~Chen.
\newblock Beyond reverse {KL:} generalizing direct preference optimization with diverse divergence constraints.
\newblock \emph{ICLR}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Xiong, Xie, Zhao, and Zhang]{ArmoRM}
H.~Wang, W.~Xiong, T.~Xie, H.~Zhao, and T.~Zhang.
\newblock Interpretable preferences via multi-objective reward modeling and mixture-of-experts.
\newblock \emph{arXiv preprint arXiv:2406.12845}, 2024{\natexlab{b}}.

\bibitem[Wu et~al.(2024{\natexlab{a}})Wu, Xie, Yang, Wu, Chen, Gao, Ding, Wang, and He]{drdpo}
J.~Wu, Y.~Xie, Z.~Yang, J.~Wu, J.~Chen, J.~Gao, B.~Ding, X.~Wang, and X.~He.
\newblock Towards robust alignment of language models: Distributionally robustifying direct preference optimization.
\newblock \emph{CoRR}, abs/2407.07880, 2024{\natexlab{a}}.

\bibitem[Wu et~al.(2024{\natexlab{b}})Wu, Sun, Yuan, Ji, Yang, and Gu]{sppo}
Y.~Wu, Z.~Sun, H.~Yuan, K.~Ji, Y.~Yang, and Q.~Gu.
\newblock Self-play preference optimization for language model alignment.
\newblock \emph{arXiv preprint arXiv:2405.00675}, 2024{\natexlab{b}}.

\bibitem[Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and Chen]{xia2024less}
M.~Xia, S.~Malladi, S.~Gururangan, S.~Arora, and D.~Chen.
\newblock Less: Selecting influential data for instruction tuning.
\newblock 2024.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, Huang, and Huang]{RRHF}
Z.~Yuan, H.~Yuan, C.~Tan, W.~Wang, S.~Huang, and F.~Huang.
\newblock {RRHF:} rank responses to align language models with human feedback without tears.
\newblock In \emph{{NeurIPS}}, 2023.

\bibitem[Zhang et~al.(2021)Zhang, Wu, Bayrooti, and Goodman]{Uncertainty}
O.~Zhang, M.~Wu, J.~Bayrooti, and N.~D. Goodman.
\newblock Temperature as uncertainty in contrastive learning.
\newblock \emph{CoRR}, abs/2110.04403, 2021.

\bibitem[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{SLiC-HF}
Y.~Zhao, R.~Joshi, T.~Liu, M.~Khalman, M.~Saleh, and P.~J. Liu.
\newblock Slic-hf: Sequence likelihood calibration with human feedback.
\newblock \emph{CoRR}, abs/2305.10425, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, Zhang, Ghosh, Lewis, Zettlemoyer, and Levy]{LIMA}
C.~Zhou, P.~Liu, P.~Xu, S.~Iyer, J.~Sun, Y.~Mao, X.~Ma, A.~Efrat, P.~Yu, L.~Yu, S.~Zhang, G.~Ghosh, M.~Lewis, L.~Zettlemoyer, and O.~Levy.
\newblock {LIMA:} less is more for alignment.
\newblock In \emph{NeurIPS}, 2023.

\end{thebibliography}
