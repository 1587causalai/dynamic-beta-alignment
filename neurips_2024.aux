\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{GPT4,llama2,GPT4_2}
\citation{instructGPT}
\citation{instructGPT}
\citation{PPO}
\citation{DPO}
\citation{outlier_dataset}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{Introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{mixture_data}
\citation{ChristianoLBMLA17,Bai2022training,llama2,instructGPT,fangjf2024neuron,FangZW00LWD024}
\citation{RAFT}
\citation{RRHF}
\citation{DPO}
\citation{SLiC-HF}
\citation{KTO}
\citation{drdpo}
\citation{RSO}
\citation{ORPO}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig_1_1}{{1a}{2}{\relax }{figure.caption.2}{}}
\newlabel{sub@fig_1_1}{{a}{2}{\relax }{figure.caption.2}{}}
\newlabel{fig_1_2}{{1b}{2}{\relax }{figure.caption.2}{}}
\newlabel{sub@fig_1_2}{{b}{2}{\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {(\ref {fig_1_1}) Pairwise Data: Low vs. High Gap}: ``Low gap'' denotes cases where the chosen and rejected examples are closely similar, typically indicating high-quality, informative pairs. ``High gap'' signifies pairs with larger differences, implying lower-quality data. \textbf  {(\ref {fig_1_2}) Influence of Data Quality on $\beta $ Selection:} Pythia-1.4B's performance on the HH dataset reveals a distinct trend: for ``Low gap'', a higher $\beta $ reduces win rate, whereas for ``High gap'', an increased $\beta $ improves it.}}{2}{figure.caption.2}\protected@file@percent }
\newlabel{fig:combined}{{1}{2}{\textbf {(\ref {fig_1_1}) Pairwise Data: Low vs. High Gap}: ``Low gap'' denotes cases where the chosen and rejected examples are closely similar, typically indicating high-quality, informative pairs. ``High gap'' signifies pairs with larger differences, implying lower-quality data. \textbf {(\ref {fig_1_2}) Influence of Data Quality on $\beta $ Selection:} Pythia-1.4B's performance on the HH dataset reveals a distinct trend: for ``Low gap'', a higher $\beta $ reduces win rate, whereas for ``High gap'', an increased $\beta $ improves it}{figure.caption.2}{}}
\citation{LIMA}
\citation{Bai2022training}
\citation{WebGPT}
\citation{Gao23ICML}
\citation{RSO}
\citation{morimura2024filtered}
\citation{ChristianoLBMLA17}
\citation{bradley1952rank}
\citation{ChristianoLBMLA17}
\citation{PPO}
\citation{DPO}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\newlabel{Related Work}{{2}{3}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Preliminaries}{3}{section.3}\protected@file@percent }
\newlabel{Preliminaries}{{3}{3}{Preliminaries}{section.3}{}}
\citation{Bai2022training}
\citation{pythia}
\citation{DPO}
\citation{DPO}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Win rate performance of DPO across different $\beta $ settings on the \emph  {low gap}, \emph  {mixed gap}, and \emph  {high gap} datasets.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig_low_high_gap}{{2}{4}{Win rate performance of DPO across different $\beta $ settings on the \emph {low gap}, \emph {mixed gap}, and \emph {high gap} datasets}{figure.caption.3}{}}
\newlabel{eq:DPO2}{{4}{4}{Preliminaries}{equation.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{4}{section.4}\protected@file@percent }
\newlabel{our_work}{{4}{4}{Method}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Motivation: The Impact of Pairwise Data Quality on $\beta $ Selection}{4}{subsection.4.1}\protected@file@percent }
\newlabel{motivation_sec}{{4.1}{4}{Motivation: The Impact of Pairwise Data Quality on $\beta $ Selection}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The distribution of individual reward discrepancy ($r(\mathbf  {y}_w^{(i)};\mathbf  {x}^{(i)})-r(\mathbf  {y}_l^{(i)};\mathbf  {x}^{(i)})$) on the training dataset of HH.}}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:gap_distribution}{{3}{5}{The distribution of individual reward discrepancy ($r(\yb _w^{(i)};\xb ^{(i)})-r(\yb _l^{(i)};\xb ^{(i)})$) on the training dataset of HH}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Method: Dynamic $\beta $ Calibration in DPO}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Dynamic $\beta $ Calibration at Batch-Level}{5}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{eq:beta}{{5}{5}{Dynamic $\beta $ Calibration at Batch-Level}{equation.4.5}{}}
\citation{ge2015escaping}
\citation{robbins1951stochastic,bottou2010large}
\citation{mae}
\citation{3sigma}
\citation{RSO,morimura2024filtered,pruthi2020estimating,f-dpo}
\citation{RSO,morimura2024filtered,LIMA,pruthi2020estimating,xia2024less}
\newlabel{mom}{{7}{6}{Dynamic $\beta $ Calibration at Batch-Level}{equation.4.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}$\beta $-Guided Data Filtering }{6}{subsubsection.4.2.2}\protected@file@percent }
\newlabel{sampling_p}{{8}{6}{$\beta $-Guided Data Filtering}{equation.4.8}{}}
\newlabel{mom2}{{9}{6}{$\beta $-Guided Data Filtering}{equation.4.9}{}}
\citation{Uncertainty}
\citation{long_tail}
\citation{MACL}
\citation{Bai2022training}
\citation{tldr_dataset}
\citation{DPO}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \textbf  {Left.} The win rates computed by GPT-4 evaluations for the Anthropic-HH one-step dialogue; $\beta $-DPO consistently outperforms across all sampling temperatures. \textbf  {Right.} In the comparison of TL;DR summarization win rates versus chosen summaries with GPT-4 as the evaluator, $\beta $-DPO is distinguished as the only strategy achieving a win rate over 50\% across different sampling temperatures. }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig:dialogue-main}{{4}{7}{\textbf {Left.} The win rates computed by GPT-4 evaluations for the Anthropic-HH one-step dialogue; $\beta $-DPO consistently outperforms across all sampling temperatures. \textbf {Right.} In the comparison of TL;DR summarization win rates versus chosen summaries with GPT-4 as the evaluator, $\beta $-DPO is distinguished as the only strategy achieving a win rate over 50\% across different sampling temperatures}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Discussion with Previous Studies}{7}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{7}{section.5}\protected@file@percent }
\newlabel{Experiments}{{5}{7}{Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Empirical Evaluation of $\beta $-DPO on Dialogue Generation and Summarization}{7}{subsection.5.1}\protected@file@percent }
\citation{pruthi2020estimating,xia2024less}
\citation{ipo}
\citation{KTO}
\citation{sppo}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Win rate comparison of Pythia-410M, -1.4B, and -2.8B models on the Anthropic HH dataset, evaluated using GPT-4. }}{8}{table.caption.6}\protected@file@percent }
\newlabel{tab:ablation}{{1}{8}{Win rate comparison of Pythia-410M, -1.4B, and -2.8B models on the Anthropic HH dataset, evaluated using GPT-4}{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Adaptations of $\beta $-DPO}{8}{subsection.5.2}\protected@file@percent }
\newlabel{exp_adaptation}{{5.2}{8}{Adaptations of $\beta $-DPO}{subsection.5.2}{}}
\citation{SimPO2024}
\citation{llama3modelcard}
\citation{Jiang2023Mistral7}
\citation{Ding2023EnhancingCL}
\citation{SimPO2024}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \textbf  {Left:} Win rates from GPT-4 evaluations on Anthropic-HH single-turn dialogues, showcasing $\beta $-DPO's adaptability to diverse filtering strategies. \textbf  {Middle:} Win rates of $\beta $-DPO across various DPO variants as evaluated by GPT-4. \textbf  {Right:} Distribution of individual reward discrepancies following fine-tuning through batch-level and instance-level calibration. }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:gradient}{{5}{9}{\textbf {Left:} Win rates from GPT-4 evaluations on Anthropic-HH single-turn dialogues, showcasing $\beta $-DPO's adaptability to diverse filtering strategies. \textbf {Middle:} Win rates of $\beta $-DPO across various DPO variants as evaluated by GPT-4. \textbf {Right:} Distribution of individual reward discrepancies following fine-tuning through batch-level and instance-level calibration}{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Comparison of win rates across varying mixture ratios on the Anthropic HH dataset, with each ratio indicating the proportion of \emph  {high-gap} to \emph  {low-gap} datasets, e.g., a 40\% mixture ratio reflects a blend of 40\% \emph  {high-gap} and 60\% \emph  {low-gap}. }}{9}{table.caption.8}\protected@file@percent }
\newlabel{tab:noise}{{2}{9}{Comparison of win rates across varying mixture ratios on the Anthropic HH dataset, with each ratio indicating the proportion of \emph {high-gap} to \emph {low-gap} datasets, e.g., a 40\% mixture ratio reflects a blend of 40\% \emph {high-gap} and 60\% \emph {low-gap}}{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Necessity of Batch-Level Dynamic $\beta $ Calibration}{9}{subsection.5.3}\protected@file@percent }
\newlabel{sec_batch_level}{{5.3}{9}{Necessity of Batch-Level Dynamic $\beta $ Calibration}{subsection.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance comparison of different models}}{9}{table.caption.9}\protected@file@percent }
\newlabel{tab:performance_comparison_lc}{{3}{9}{Performance comparison of different models}{table.caption.9}{}}
\citation{spin,sppo}
\bibdata{neurips_2024}
\bibcite{llama3modelcard}{{1}{2024}{{AI@Meta}}{{}}}
\bibcite{ipo}{{2}{2023}{{Azar et~al.}}{{Azar, Rowland, Piot, Guo, Calandriello, Valko, and Munos}}}
\bibcite{Bai2022training}{{3}{2022}{{Bai et~al.}}{{Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, Showk, Elhage, Hatfield{-}Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda, Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and Kaplan}}}
\bibcite{outlier_dataset}{{4}{2023}{{Bejan et~al.}}{{Bejan, Sokolov, and Filippova}}}
\bibcite{pythia}{{5}{2023}{{Biderman et~al.}}{{Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der Wal}}}
\bibcite{bottou2010large}{{6}{2010}{{Bottou}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion and Future Work}{10}{section.6}\protected@file@percent }
\newlabel{conclusion}{{6}{10}{Conclusion and Future Work}{section.6}{}}
\bibcite{bradley1952rank}{{7}{1952}{{Bradley and Terry}}{{}}}
\bibcite{GPT4_2}{{8}{2023}{{Bubeck et~al.}}{{Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, Nori, Palangi, Ribeiro, and Zhang}}}
\bibcite{spin}{{9}{2024}{{Chen et~al.}}{{Chen, Deng, Yuan, Ji, and Gu}}}
\bibcite{ChristianoLBMLA17}{{10}{2017}{{Christiano et~al.}}{{Christiano, Leike, Brown, Martic, Legg, and Amodei}}}
\bibcite{Ding2023EnhancingCL}{{11}{2023}{{Ding et~al.}}{{Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou}}}
\bibcite{RAFT}{{12}{2023}{{Dong et~al.}}{{Dong, Xiong, Goyal, Pan, Diao, Zhang, Shum, and Zhang}}}
\bibcite{KTO}{{13}{2024}{{Ethayarajh et~al.}}{{Ethayarajh, Xu, Muennighoff, Jurafsky, and Kiela}}}
\bibcite{fangjf2024neuron}{{14}{2024{}}{{Fang et~al.}}{{Fang, Bi, Wang, Jiang, Gao, Wang, Zhang, Shi, Wang, and Chua}}}
\bibcite{FangZW00LWD024}{{15}{2024{}}{{Fang et~al.}}{{Fang, Zhang, Wu, Yang, Liu, Li, Wang, Du, and Wang}}}
\bibcite{Gao23ICML}{{16}{2023}{{Gao et~al.}}{{Gao, Schulman, and Hilton}}}
\bibcite{ge2015escaping}{{17}{2015}{{Ge et~al.}}{{Ge, Huang, Jin, and Yuan}}}
\bibcite{havrilla-etal-2023-trlx}{{18}{2023}{{Havrilla et~al.}}{{Havrilla, Zhuravinskyi, Phung, Tiwari, Tow, Biderman, Anthony, and Castricato}}}
\bibcite{ORPO}{{19}{2024}{{Hong et~al.}}{{Hong, Lee, and Thorne}}}
\bibcite{MACL}{{20}{2023}{{Huang et~al.}}{{Huang, Chen, Wen, Zhang, Li, Wang, and Chen}}}
\bibcite{Jiang2023Mistral7}{{21}{2023{}}{{Jiang et~al.}}{{Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~Las~Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed}}}
\bibcite{Jiang2023LLMBlenderEL}{{22}{2023{}}{{Jiang et~al.}}{{Jiang, Ren, and Lin}}}
\bibcite{long_tail}{{23}{2023}{{Kukleva et~al.}}{{Kukleva, B{\"{o}}hle, Schiele, Kuehne, and Rupprecht}}}
\bibcite{mae}{{24}{2016}{{Lillicrap et~al.}}{{Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra}}}
\bibcite{RSO}{{25}{2023}{{Liu et~al.}}{{Liu, Zhao, Joshi, Khalman, Saleh, Liu, and Liu}}}
\bibcite{SimPO2024}{{26}{2024}{{Meng et~al.}}{{Meng, Xia, and Chen}}}
\bibcite{morimura2024filtered}{{27}{2024}{{Morimura et~al.}}{{Morimura, Sakamoto, Jinnai, Abe, and Air}}}
\bibcite{WebGPT}{{28}{2021}{{Nakano et~al.}}{{Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse, Jain, Kosaraju, Saunders, Jiang, Cobbe, Eloundou, Krueger, Button, Knight, Chess, and Schulman}}}
\bibcite{GPT4}{{29}{2023}{{OpenAI}}{{}}}
\bibcite{instructGPT}{{30}{2022}{{Ouyang et~al.}}{{Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe}}}
\bibcite{mixture_data}{{31}{2023}{{Penedo et~al.}}{{Penedo, Malartic, Hesslow, Cojocaru, Alobeidli, Cappelli, Pannier, Almazrouei, and Launay}}}
\bibcite{pruthi2020estimating}{{32}{2020}{{Pruthi et~al.}}{{Pruthi, Liu, Kale, and Sundararajan}}}
\bibcite{3sigma}{{33}{1994}{{Pukelsheim}}{{}}}
\bibcite{DPO}{{34}{2023}{{Rafailov et~al.}}{{Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn}}}
\bibcite{robbins1951stochastic}{{35}{1951}{{Robbins and Monro}}{{}}}
\bibcite{PPO}{{36}{2017}{{Schulman et~al.}}{{Schulman, Wolski, Dhariwal, Radford, and Klimov}}}
\bibcite{tldr2}{{37}{2020}{{Stiennon et~al.}}{{Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano}}}
\bibcite{llama2}{{38}{2023}{{Touvron et~al.}}{{Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton{-}Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}}}
\bibcite{tldr_dataset}{{39}{2017}{{V{\"{o}}lske et~al.}}{{V{\"{o}}lske, Potthast, Syed, and Stein}}}
\bibcite{f-dpo}{{40}{2024{}}{{Wang et~al.}}{{Wang, Jiang, Yang, Liu, and Chen}}}
\bibcite{ArmoRM}{{41}{2024{}}{{Wang et~al.}}{{Wang, Xiong, Xie, Zhao, and Zhang}}}
\bibcite{drdpo}{{42}{2024{}}{{Wu et~al.}}{{Wu, Xie, Yang, Wu, Chen, Gao, Ding, Wang, and He}}}
\bibcite{sppo}{{43}{2024{}}{{Wu et~al.}}{{Wu, Sun, Yuan, Ji, Yang, and Gu}}}
\bibcite{xia2024less}{{44}{2024}{{Xia et~al.}}{{Xia, Malladi, Gururangan, Arora, and Chen}}}
\bibcite{RRHF}{{45}{2023}{{Yuan et~al.}}{{Yuan, Yuan, Tan, Wang, Huang, and Huang}}}
\bibcite{Uncertainty}{{46}{2021}{{Zhang et~al.}}{{Zhang, Wu, Bayrooti, and Goodman}}}
\bibcite{SLiC-HF}{{47}{2023}{{Zhao et~al.}}{{Zhao, Joshi, Liu, Khalman, Saleh, and Liu}}}
\bibcite{LIMA}{{48}{2023}{{Zhou et~al.}}{{Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, Zhang, Ghosh, Lewis, Zettlemoyer, and Levy}}}
\bibstyle{abbrvnat}
\citation{tldr2}
\citation{havrilla-etal-2023-trlx}
\citation{tldr2}
\@writefile{toc}{\contentsline {section}{\numberline {A}Experiment}{14}{appendix.A}\protected@file@percent }
\newlabel{exp_appendix}{{A}{14}{Experiment}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}$\beta $-DPO Implementation Details and Hyperparameters}{14}{subsection.A.1}\protected@file@percent }
\newlabel{setup_exp}{{A.1}{14}{$\beta $-DPO Implementation Details and Hyperparameters}{subsection.A.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces $\beta $-Direct Preference Optimization}}{14}{algorithm.1}\protected@file@percent }
\newlabel{alg:beta-dpo}{{1}{14}{$\beta $-Direct Preference Optimization}{algorithm.1}{}}
\newlabel{alg}{{1}{14}{$\beta $-Direct Preference Optimization}{ALC@unique.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Mixture of \emph  {low gap} and \emph  {high gap}}{14}{subsection.A.2}\protected@file@percent }
\newlabel{low_gap_high_gap}{{A.2}{14}{Mixture of \emph {low gap} and \emph {high gap}}{subsection.A.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Distribution of individual reward discrepancies following the Pythia-2.8B model. }}{15}{figure.caption.12}\protected@file@percent }
\newlabel{fig_mix}{{6}{15}{Distribution of individual reward discrepancies following the Pythia-2.8B model}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Performance comparison across different $\beta $ values and $\rho $ values for three different model sizes (Pythia-2.8B, Pythia-1.4B, and Pythia-410M) on the Anthropic HH dataset using GPT-4 as the evaluator. Each subplot represents the win rate for varying parameters $\beta $ = 0.1, 0.3, and 0.9 with exponential smoothing. }}{15}{figure.caption.13}\protected@file@percent }
\newlabel{fig_params}{{7}{15}{Performance comparison across different $\beta $ values and $\rho $ values for three different model sizes (Pythia-2.8B, Pythia-1.4B, and Pythia-410M) on the Anthropic HH dataset using GPT-4 as the evaluator. Each subplot represents the win rate for varying parameters $\beta $ = 0.1, 0.3, and 0.9 with exponential smoothing}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Hyperparameter Sensitivity}{15}{subsection.A.3}\protected@file@percent }
\citation{mae}
\citation{Jiang2023LLMBlenderEL}
\citation{SimPO2024}
\citation{ArmoRM}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}The ablation study \emph  {w.r.t. }$M_0$}{16}{subsection.A.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Comparison of win rates across varying $M_0$ in $\beta $-DPO. }}{16}{table.caption.14}\protected@file@percent }
\newlabel{tab:constant_m0}{{4}{16}{Comparison of win rates across varying $M_0$ in $\beta $-DPO}{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Our Methods with Explicit Reward Model}{16}{subsection.A.5}\protected@file@percent }
\newlabel{sec:appendix_explicit_rm}{{A.5}{16}{Our Methods with Explicit Reward Model}{subsection.A.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of different methods on Llama3-Instruct (8B) with explicit reward model}}{16}{table.caption.15}\protected@file@percent }
\newlabel{tab:comparison}{{5}{16}{Comparison of different methods on Llama3-Instruct (8B) with explicit reward model}{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}GPT-4 prompts for computing summarization and dialogue win rates}{17}{subsection.A.6}\protected@file@percent }
\newlabel{app:prompts}{{A.6}{17}{GPT-4 prompts for computing summarization and dialogue win rates}{subsection.A.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Broader Impacts}{17}{appendix.B}\protected@file@percent }
\newlabel{broader_impacts}{{B}{17}{Broader Impacts}{appendix.B}{}}
\gdef \@abspage@last{23}
